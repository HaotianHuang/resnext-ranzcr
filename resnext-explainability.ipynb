{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ResNext Model for Chest XRAY Catheter and Line Detection Challenge","metadata":{"id":"CDbpH8nnBhdx"}},{"cell_type":"markdown","source":"**Description of challenge**: Classify the presence and correct placement of tubes on chest x-rays to save lives\n\n**Dataset**: CLiP, catheter and line position dataset[1]\n\n**Model Architecture**: resnext50_32x4d\n\n**Performance**: 93% accuracy using 4-fold cross-validation (comparable to state of the art models in literature[2])\n\n**Explainability**: Using occlusion-based attribution to see regions of high importance to the model's decision-making process, thus adding interpretability\n\n**Purpose**: This notebook goes through the steps for explaining this model. It is accompanied by the resnext-training.ipynb notebook which shows the training and evaluating process.  \n\n**Notes for use**: notebook assumes kaggle environment and .cuda() availability. \n\n*[1]: https://www.nature.com/articles/s41597-021-01066-8.pdf*\n\n*[2]: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8017400/pdf/ryai.2020190082.pdf*","metadata":{"id":"ozcCavo0Bhd0"}},{"cell_type":"markdown","source":"# 1 - Setup","metadata":{}},{"cell_type":"code","source":"!pip install gdown\n!pip install captum\n!pip install timm\n!gdown 1dWwapuVx_aK6K2j5Jj1St2F7RTQ9DirG\n!gdown 1g-pijm1Db9cfOxBhHw5h5aqVnCEWjtTL #this is cvc-normal from train set\n!gdown 1CalZo9FPVKDHZ-66pe2CqWoZm94X4j8m #this is cvc-abnormal from train set predicted incorrectly\n!gdown 1mjBY-yXXIzZhaLCAC5dfxBshF68GjCMM #cvc-abnormal train set","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:25:57.284059Z","iopub.execute_input":"2022-08-23T12:25:57.284715Z","iopub.status.idle":"2022-08-23T12:26:48.661004Z","shell.execute_reply.started":"2022-08-23T12:25:57.284676Z","shell.execute_reply":"2022-08-23T12:26:48.659873Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom PIL import Image\n\nimport os\nimport json\nimport numpy as np\nfrom matplotlib.colors import LinearSegmentedColormap\nimport pandas as dp\n\nimport torchvision\nfrom torchvision import models\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom captum.attr import IntegratedGradients\nfrom captum.attr import GradientShap\nfrom captum.attr import Occlusion\nfrom captum.attr import NoiseTunnel\nfrom captum.attr import visualization as viz\nimport timm\n\nfrom albumentations import (\n    Compose, OneOf, Normalize, Resize, RandomResizedCrop, RandomCrop, HorizontalFlip, VerticalFlip, \n    RandomBrightness, RandomContrast, RandomBrightnessContrast, Rotate, ShiftScaleRotate, Cutout, \n    IAAAdditiveGaussianNoise, Transpose\n    )\nfrom albumentations.pytorch import ToTensorV2\nfrom albumentations import ImageOnlyTransform\n\n%cd ..\n","metadata":{"id":"ashEs63bBueT","outputId":"9250c4c1-9710-4906-9b83-5fb7b4941346","execution":{"iopub.status.busy":"2022-08-23T12:27:30.370466Z","iopub.execute_input":"2022-08-23T12:27:30.370842Z","iopub.status.idle":"2022-08-23T12:27:30.383181Z","shell.execute_reply.started":"2022-08-23T12:27:30.370813Z","shell.execute_reply":"2022-08-23T12:27:30.382184Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class CustomResNext(nn.Module):\n    def __init__(self, model_name='resnext50_32x4d', pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        n_features = self.model.fc.in_features\n        self.model.fc = nn.Linear(n_features, 11)\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\nmodel = CustomResNext()\nif torch.cuda.is_available():\n    model.cuda()\nweights = torch.load('working/resnext50_32x4d_fold2_best-21aug-7hrs.pth')\nmodel.load_state_dict(weights, strict=False)\nmodel.eval()\n\ntarget_cols=['ETT - Abnormal', 'ETT - Borderline', 'ETT - Normal',\n             'NGT - Abnormal', 'NGT - Borderline', 'NGT - Incompletely Imaged', 'NGT - Normal', \n             'CVC - Abnormal', 'CVC - Borderline', 'CVC - Normal',\n             'Swan Ganz Catheter Present']","metadata":{"id":"FKfNMHtYMnZ7","execution":{"iopub.status.busy":"2022-08-23T12:27:32.973503Z","iopub.execute_input":"2022-08-23T12:27:32.974175Z","iopub.status.idle":"2022-08-23T12:27:36.322049Z","shell.execute_reply.started":"2022-08-23T12:27:32.974140Z","shell.execute_reply":"2022-08-23T12:27:36.320881Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#CVC - abnormal \nimg = Image.open('working/1.2.826.0.1.3680043.8.498.68286643202323212801283518367144358744.jpg').convert('RGB')\nnormalize_transform = torchvision.transforms.Compose([\n    torchvision.transforms.Resize(600),\n    torchvision.transforms.ToTensor(),\n    torchvision.transforms.Normalize(mean = (0.485, 0.456, 0.406), \n                                     std = (0.229, 0.224, 0.225))])\n\n\ninput = normalize_transform(img).unsqueeze(0)\nprint(input.shape)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:27:38.779870Z","iopub.execute_input":"2022-08-23T12:27:38.780431Z","iopub.status.idle":"2022-08-23T12:27:38.989311Z","shell.execute_reply.started":"2022-08-23T12:27:38.780383Z","shell.execute_reply":"2022-08-23T12:27:38.988338Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"output = model(input.cuda())\noutput = F.softmax(output, dim=1)\nprediction_score, pred_label_idx = torch.topk(output, 1)\npredicted_label = target_cols[pred_label_idx.item()]\nprint('Predicted:', predicted_label, '(', prediction_score.squeeze().item(), ')')\n","metadata":{"id":"Xyi2vMLwBhd7","outputId":"c0efac88-3004-477f-f484-a06a929aed1d","execution":{"iopub.status.busy":"2022-08-23T12:27:41.081950Z","iopub.execute_input":"2022-08-23T12:27:41.082447Z","iopub.status.idle":"2022-08-23T12:27:46.752634Z","shell.execute_reply.started":"2022-08-23T12:27:41.082397Z","shell.execute_reply":"2022-08-23T12:27:46.751406Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# To clear GPU memory - use only if you need to restart\nfrom numba import cuda\ncuda.select_device(0)\ncuda.close()\ncuda.select_device(0)","metadata":{"execution":{"iopub.status.busy":"2022-08-23T12:23:10.500107Z","iopub.execute_input":"2022-08-23T12:23:10.500511Z","iopub.status.idle":"2022-08-23T12:23:12.514156Z","shell.execute_reply.started":"2022-08-23T12:23:10.500461Z","shell.execute_reply":"2022-08-23T12:23:12.513215Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## 2- Gradient-based attribution","metadata":{"id":"zHUp39vVBhd7"}},{"cell_type":"code","source":"occlusion = Occlusion(model)\n\nattributions_occ = occlusion.attribute(input.cuda(),\n                                       strides = (3, 30, 30),\n                                       target=pred_label_idx,\n                                       sliding_window_shapes=(3,45, 45),\n                                       baselines=0,\n                                       show_progress=True)\n\n_ = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                      np.transpose(input.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                      [\"original_image\", \"heat_map\"],\n                                      [\"all\", \"positive\"],\n                                      show_colorbar=True,\n                                      outlier_perc=2,\n                                     )\n","metadata":{"id":"8_lODYz0Bhd-","execution":{"iopub.status.busy":"2022-08-23T12:28:28.149000Z","iopub.execute_input":"2022-08-23T12:28:28.149373Z","iopub.status.idle":"2022-08-23T12:28:44.333085Z","shell.execute_reply.started":"2022-08-23T12:28:28.149343Z","shell.execute_reply":"2022-08-23T12:28:44.331876Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"occlusion = Occlusion(model)\n\nattributions_occ = occlusion.attribute(input.cuda(),\n                                       strides = (3, 50, 50),\n                                       target=pred_label_idx,\n                                       sliding_window_shapes=(3,60, 60),\n                                       baselines=0,\n                                       show_progress=True)\n\n_ = viz.visualize_image_attr_multiple(np.transpose(attributions_occ.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                      np.transpose(input.squeeze().cpu().detach().numpy(), (1,2,0)),\n                                      [\"original_image\", \"heat_map\"],\n                                      [\"all\", \"positive\"],\n                                      show_colorbar=True,\n                                      outlier_perc=2,\n                                     )","metadata":{"id":"eeCJrvzyBhd-","outputId":"0b7ae3c7-9b10-47ab-da41-fcf7abbe367d","execution":{"iopub.status.busy":"2022-08-23T12:28:45.714992Z","iopub.execute_input":"2022-08-23T12:28:45.715378Z","iopub.status.idle":"2022-08-23T12:28:52.078100Z","shell.execute_reply.started":"2022-08-23T12:28:45.715345Z","shell.execute_reply":"2022-08-23T12:28:52.077153Z"},"trusted":true},"execution_count":11,"outputs":[]}]}